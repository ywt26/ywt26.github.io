%% LyX 2.3.2-2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt,english]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{verbose,bmargin=1.25in,lmargin=1.2in,rmargin=1.2in}
\setlength{\parindent}{0in}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{setspace}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
\numberwithin{figure}{section}

\@ifundefined{date}{}{\date{}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{CJK}

\makeatother

\usepackage{babel}
\begin{document}
\begin{CJK}{UTF8}{gbsn}
\title{Brief Introduction of Bayesian Estimation}
\author{Yao Weitong}

\maketitle
Bayesian statistics is a huge system. Different authors have slightly
different interpretations of Bayesian estimation. This article is
a concise synthesis of many textbooks, contains the recommended chapters
in those books, and is a brief introduction to Bayesian estimates
from the perspective of non-statistical major. You can follow the
textbooks listed in the Reference for in-depth study.

\section{Importance of prior probability}

As an example, when a guy purchases a life insurance, the insurance
company will estimate the probability of the insured's risk and determine
the premium. According to the classical frequentists, the person is
considered as a certain individual of the sample with same features
of age, income, etc.. However, information asymmetry always arises,
and that’s the limitation of excluding the personal prior information
of the insured. If the insurance company fails to conduct a prior
investigation, the insured may have individual differences compared
with the sample; and the investigation of the medical records and
living habits of the insured in the past is capable to update the
estimated probability. Another example stresses the critical role
played by prior probabilities in estimation. The mathematician named
John Craven proposed a search scheme for a successful search of lost
USS Scorpion (SSN-589), using the Bayesian formula. He used the probability
map of the sea area, which was divided into many small squares with
two probability p and q. p is the probability that the submarine lies
in this square. q is the probability that it was searched if the submarine
is in this grid. If a grid is searched and no trace of the submarine
is found, then according to the Bayesian formula, the probability
of existence of the lattice submarine will decrease$p'=\frac{p(1-q)}{(1-p)+p(1-q)}$
; since the sum of probabilities is 1, then the probability value
of other grid will be rise$r'=\frac{r}{1-pq}>r$ (change in prior
probability)\cite{1}.

\subsection{Frequentists and Bayesians}
\begin{itemize}
\item For standard Frequentists, probability requires a lot of repeated
sampling and the parameters are considered fixed but unknown. Frequency
indicates probability. The Bayesians believe that the data should
be fixed, the parameters are random, the frequency is a description
of the degree of uncertainty, and the probability should be based
on our understanding of the world or previous studies.
\item In practice, Frequentists determine the specific values (e.g. expectations,
variances, etc.) of the parameters by optimization criteria (e.g.
maximize likelihood function), and derive conclusions from a single
data set. In the Bayesian view, given the observation data, the first
step is to model a prior distribution, and then repeatedly apply Bayes'
theorem, combine the data set (as a prior probability distribution),
and continuously update the probability to calculate posterior probability
distribution. The result is a distribution rather than a specific
value, so it needs sampling for further statistical inference.
\end{itemize}
Take an example; suppose we want to study the influence of Trump's
Twitter content on the exchange rate of US dollars against RMB. Firstly,
the text analysis method is used to extract the keyword frequency
of the emotional judgment and intention, and regressed on the exchange
rate fluctuation of the corresponding time period. This is what the
Frequentists usually do. However, Bayesians believe that all other
factors that affect exchange rate movements should be considered to
construct a prior probability distribution (using the same data set
of other factors used by Frequentists), and the content of the Twitter
is used to update beliefs\cite{2}.

\section{Bayes's Theorem and Bayesian Estimation}

For a random vector \ensuremath{\theta} (as parameters) and random
vector \ensuremath{D} (as data set), the Bayes's Theorem indicates:

\[
\text{\ensuremath{p}}(\theta\mid D)=\dfrac{\text{\ensuremath{p}}(\text{\ensuremath{D\mid\theta}})\text{\ensuremath{p}}(\text{\ensuremath{\theta}})}{\text{\ensuremath{p}}(\text{\ensuremath{D}})}
\]
$\text{\ensuremath{p}}(\text{\ensuremath{\theta\mid D}})$is the posterior
probability distribution; $\text{\ensuremath{p}}(\text{\ensuremath{D\mid\theta}})$
is considered the function of \ensuremath{\theta}, always denoted
as likelihood function$\text{\ensuremath{L}}(\text{\ensuremath{\theta}; \ensuremath{D}})$,
indicating the probability of observation under certain \ensuremath{\theta};
and $\text{\ensuremath{p}}(\text{\ensuremath{\theta}})$ is prior
probability distribution. The denominator $\text{\ensuremath{p}}(\text{\ensuremath{D}})$
is a normalized constant, which makes sure posterior probability integral
equal to 1, and posterior distribution can be directly proportional
to the numerator(or said ``density kernel''): 
\[
posterior\propto likelihood\times prior
\]


\subsection{Example of Gauss Distribution Estimation\cite{3}\cite{4}}

Suppose the random sample is $\text{\ensuremath{D}}=(d1,d2,...,dn)'$,
where \ensuremath{D} is the Gaussian distribution with known variance
$\sigma^{2}$ and unknown mean $\text{\ensuremath{\mu}}$, find the
mean$\text{\ensuremath{\mu}}$. Frequentists use MLE to solve the
problem, whilst Bayesians prefer to applying prior distribution and
implementing with three steps.

\subsubsection*{1. Model Prior Distribution \textit{\ensuremath{p}}(\textit{\ensuremath{\theta}}):}

Since target distribution is normal and conjugative, it's reasonable
to set $\text{\ensuremath{\theta}}\sim N(\text{\ensuremath{\mu}}_{0},\sigma_{0}^{2})$;
define \textbf{\textit{presion}} of $\text{\ensuremath{\theta}}$
as follows, the larger the \textit{presion} $h$ is, the smaller the
$\text{\ensuremath{\sigma_{0}^{2}}}$ will be.
\[
h\equiv\frac{1}{\text{\ensuremath{\sigma}}_{0}^{2}}
\]
the prior distribution of $\text{\ensuremath{\theta}}$ is:
\[
\text{\ensuremath{p}}(\text{\ensuremath{\theta}})=\dfrac{1}{\sqrt{2\text{\ensuremath{\pi}}\sigma_{0}^{2}}}exp\left\{ -(\theta-\text{\ensuremath{\mu}}_{0})^{2}/2\text{\ensuremath{\sigma}}_{0}^{2}\right\} \:\propto\:exp\left\{ -h(\text{\ensuremath{\theta}}-\text{\ensuremath{\mu}}_{0})^{2}/2\right\} 
\]


\subsubsection*{2. Model Likelihood Function \textit{L}(\textit{\ensuremath{\theta};
\ensuremath{D})}(i.i.d):}

\[
\text{\ensuremath{L}}(\text{\ensuremath{\theta}; \ensuremath{D}})=\prod_{i=1}^{n}(2\text{\ensuremath{\pi}}\sigma^{2})^{-1/2}exp\left\{ -(d_{i}-\text{\ensuremath{\theta}})^{2}/2\sigma^{2}\right\} 
\]

\[
=(2\text{\ensuremath{\pi}}\text{\ensuremath{\sigma}}^{2})^{-n/2}exp\left\{ -\sum_{i=1}^{n}(d_{i}-\text{\ensuremath{\theta}})^{2}/2\text{\ensuremath{\sigma}}^{2}\right\} 
\]

\[
\propto exp\left\{ -\sum_{i=1}^{n}(\bar{d}-\text{\ensuremath{\theta}})^{2}/2\text{\ensuremath{\sigma}}^{2}\right\} \:\propto\:exp\left\{ -h^{*}(\bar{d}-\text{\ensuremath{\theta}})^{2}/2\right\} 
\]
where $h^{*}=n/\text{\ensuremath{\sigma}}^{2}$. Obviously, the likelihood
function (rather a PDF of \ensuremath{\theta} ) has similar formation
as prior distribution, they are conjugate distribution. Since the
multiplication of Gaussian functions is the summation of exponents,
thus posterior probability is also a Gaussian distribution.

\subsubsection*{3. Model Posterior Distribution \textit{\ensuremath{p}}(\textit{\ensuremath{\theta}}
| \textit{D}) = \textit{N}(\textit{\ensuremath{\theta}} | $\text{\ensuremath{\mu}}_{N},\text{\ensuremath{\sigma}}_{N}^{2}$)}

\[
\text{\ensuremath{p}}(\text{\ensuremath{\theta\mid D}})\:\propto\:\text{\ensuremath{L}}(\text{\ensuremath{\theta}; \ensuremath{D}})\text{\ensuremath{p}}(\text{\ensuremath{\theta}})
\]

\[
\propto exp\left\{ -h^{*}(\bar{d}-\text{\ensuremath{\theta}})^{2}/2\right\} \,\text{\ensuremath{\centerdot}}\,exp\left\{ -h(\text{\ensuremath{\theta}}-\text{\ensuremath{\mu}})^{2}/2\right\} 
\]

\[
=exp\left\{ -\dfrac{1}{2}\left[h^{*}(\bar{d}-\theta)^{2}+h(\text{\ensuremath{\theta}}-\text{\ensuremath{\mu}})^{2}\right]\right\} \:\propto\:\bar{h}(\text{\ensuremath{\theta}}-\bar{\mu})^{2}
\]
where $\bar{h}=h+h^{*},\bar{\text{ \ensuremath{\mu}}}\equiv(h\text{\ensuremath{\mu}}+h^{*}\bar{d})/\bar{h}$,
another forms are:

\[
\text{\ensuremath{\mu}}_{N}=\bar{\text{\ensuremath{\mu}}}=\dfrac{\text{\ensuremath{\sigma}}^{2}}{N\text{\ensuremath{\sigma}}_{0}^{2}+\text{\ensuremath{\sigma}}^{2}}\text{\ensuremath{\mu}}_{0}+\dfrac{N\text{\ensuremath{\text{\ensuremath{\sigma}}_{0}^{2}}}}{N\text{\ensuremath{\sigma}}_{0}^{2}+\text{\ensuremath{\sigma}}^{2}}\text{\ensuremath{\mu}}_{ML}
\]

\[
\bar{h}=\dfrac{1}{\text{\ensuremath{\sigma}}_{N}^{2}}=\dfrac{1}{\text{\ensuremath{\sigma}}_{0}^{2}}+\dfrac{N}{\text{\ensuremath{\sigma}}^{2}}
\]
From the inference shown above, it's clear that $\text{\ensuremath{\mu}}_{N}\in\left[\text{ \ensuremath{\mu}}_{0},\text{\ensuremath{\mu}}_{ML}\right]$,
when$N\rightarrow0,\text{\ensuremath{\mu}}_{N}\rightarrow\text{\ensuremath{\mu}}_{0}$;
when $N\rightarrow\infty,\text{\ensuremath{\mu}}_{N}\rightarrow\text{\ensuremath{\mu}}_{ML}$,
at the same time, $\bar{h}\rightarrow\text{\ensuremath{\infty}}$
and $\text{\ensuremath{\sigma}}_{N}^{2}\rightarrow0$. When $N$ tends
to be infinite, it connotes the presion of the sample is more and
more important for the posterior presion, and the posterior distribution
is almost not affected by the prior distribution, which helps to offset
the shortcomings of subjective prior distribution. Another question
is how we can calculate the posterior probability distribution.

\subsection{Estimation of Posterior Probability Distribution\cite{5}\cite{6}}

\subsubsection{\textit{Monte Carlo Integral}}

\noindent Let $\theta^{(s)}$ for $s=1,...,S$ be a random sample
from $\text{\ensuremath{p}}(\text{\ensuremath{\theta\mid D}})$, and
define $\hat{I}_{MC}=\frac{1}{S}\sum_{s=1}^{S}f(\theta^{(s)})$, then
$\hat{f}_{s}$ converges to $E\text{\ensuremath{\left[f(\theta)\mid D\right]}}$
as $S$ goes to infinity.

\noindent When the posterior distribution has an analytical formula,
the Monte Carlo integral method is commonly used for the posterior
mean calculation.Using the theorem, it allows to approximate $E\text{\ensuremath{\left[f(\theta)\mid D\right]}}$,
and the numerical standard error

\noindent 
\[
\sqrt{S}\left\{ \hat{f}_{s}-E\text{\ensuremath{\left[f(\theta)\mid D\right]}}\right\} \rightarrow N(0,\text{\ensuremath{\sigma}}_{f}^{2})
\]

\noindent can ensure the difference is sufficiently small.

\subsubsection{\textit{Markov chain Monte Carlo (Gibbs sampler)}}

When the integrals of posterior distribution is so complex or it has
no analytical formula, it's necceary to derive a random sample from
posterior distribution for further inference. One of popular sampler
is \textbf{\textit{Markov chain Monte Carlo sampling}}.

\noindent Suppose the distribution of $X=(X_{1},X_{2},\text{...,\ensuremath{X_{n}}})$
is $f(x\text{)}$, for any fixed $T\text{\ensuremath{\in}}N\left\{ 1,2,...,n\right\} $,
given $X_{-T}=x_{-T},$define $\tilde{X}=(\tilde{x}_{1}',\tilde{x}_{2},\text{...,\ensuremath{\tilde{x}_{n}}})$,
for any testible $B$:

\[
P(\tilde{X}\text{\ensuremath{\in}}B)=\int_{B}\pi(\tilde{x}_{-T})\text{\ensuremath{\pi}}(\tilde{x}_{-T}\mid\tilde{x}_{-T})d\tilde{x}=\int_{B}\text{\ensuremath{\pi}}(\text{\ensuremath{\pi}}(\tilde{x})d\tilde{x}=\text{\ensuremath{\pi}}(B)
\]
thus, from $X$ to $X'$, they have stationary distributions (the
PDF is unchanged), and it's called \textbf{\textit{Gibbs sampler.}}
\begin{thebibliography}{1}
\begin{spacing}{0.15}
\bibitem{1}{\footnotesize{}邓一硕,关菁菁,刘辰昂,邱怡轩,施涛,熊熹,周祺. 统计之都创作小组: 失联搜救中的统计数据分析.}{\footnotesize\par}

{\footnotesize{}\bibitem{2}Jakevdp(Jake Vanderplas) }\textit{\footnotesize{}.}{\footnotesize{}2014}\textit{\footnotesize{}.Frequentism
and Bayesianism: A Practical Introduction.}{\footnotesize{}GitHub.}{\footnotesize\par}

{\footnotesize{}\bibitem{3}陈强编.2010.《高级计量经济学及stata应用》,Chapter19,31.}{\footnotesize\par}

{\footnotesize{}\bibitem{4}Bishop, C. M. 2011. }\textit{\footnotesize{}Pattern
Recognition and Machine Learning. }{\footnotesize{}Springer.}{\footnotesize\par}

{\footnotesize{}\bibitem{5}Gary Koop. 2003. }\textit{\footnotesize{}Bayesian
Economics.}{\footnotesize\par}

{\footnotesize{}\bibitem{6}朱明惠, 林静著.《贝叶斯计量经济学模型》}{\footnotesize\par}
\end{spacing}
\end{thebibliography}
\end{CJK}
\end{document}
